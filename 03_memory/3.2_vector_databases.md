## 3.2 向量数据库选型：Pinecone、Weaviate、Chroma

向量数据库是智能体长期记忆的核心基础设施。它们专门优化了向量（embeddings）的存储和相似性搜索，是构建 RAG 系统和记忆系统的关键组件。

#### 为什么需要向量数据库

### 3.2.1 传统数据库的局限

传统数据库擅长精确匹配查询（如 `SELECT * FROM users WHERE name = 'Alice'`），但无法处理语义相似性搜索：

```
用户问："如何在 Python 中读取文件？"
相关记忆：
  - "使用 open() 函数打开文件"      ← 语义相关
  - "Python 文件操作的最佳实践"     ← 语义相关
  - "用户昨天问了文件权限问题"      ← 可能相关
```

#### 向量数据库的工作原理

```
文本 → Embedding 模型 → 向量 [0.23, -0.45, 0.67, ...] → 向量数据库

查询时:
查询文本 → 向量 → 相似性搜索（余弦相似度/欧氏距离）→ Top-K 结果
```

### 3.2.2 主流向量数据库对比

### 3.2.3 快速对比表

| 特性 | Pinecone | Weaviate | Chroma | Milvus | Qdrant |
|------|----------|----------|--------|--------|--------|
| 部署模式 | 全托管 | 自托管/云 | 本地/云 | 自托管/云 | 自托管/云 |
| 开源 | ❌ | ✅ | ✅ | ✅ | ✅ |
| 上手难度 | 简单 | 中等 | 最简单 | 较复杂 | 中等 |
| 扩展性 | 优秀 | 良好 | 有限 | 优秀 | 良好 |
| 成本 | 按用量付费 | 免费/企业版 | 免费 | 免费/企业版 | 免费/企业版 |
| 适用场景 | 生产环境 | 灵活场景 | 开发/原型 | 大规模 | 中小规模 |

---

### 3.2.4 Pinecone

#### 特点

- **全托管服务**：无需运维
- **高性能**：毫秒级查询响应
- **易用性**：简洁的 API 设计
- **Serverless**：自动扩缩容

### 3.2.5 快速开始

```python
from pinecone import Pinecone

# 初始化
pc = Pinecone(api_key="your-api-key")

# 创建索引
pc.create_index(
    name="agent-memory",
    dimension=1536,  # OpenAI ada-002 维度
    metric="cosine"
)

# 获取索引
index = pc.Index("agent-memory")

# 插入向量
index.upsert(
    vectors=[
        {
            "id": "mem_001",
            "values": [0.1, 0.2, ...],  # 1536维向量
            "metadata": {
                "text": "用户偏好：深色主题",
                "type": "preference",
                "timestamp": "2024-01-15"
            }
        }
    ]
)

# 查询
results = index.query(
    vector=[0.15, 0.3, ...],
    top_k=5,
    include_metadata=True
)
```

### 3.2.6 价格（参考）

| 计划 | 价格 | 适用场景 |
|------|------|----------|
| Starter | 免费 | 开发测试，100K 向量 |
| Standard | $0.025/小时 起 | 小型生产环境 |
| Enterprise | 定制 | 大规模部署 |

### 3.2.7 适用场景

✅ 快速上线、无运维团队、中小规模应用
❌ 需要自托管、预算有限、超大规模

---

### 3.2.8 Weaviate

#### 特点

- **开源**：Apache 2.0 许可
- **GraphQL API**：灵活的查询接口
- **模块化**：内置 Embedding 模块
- **混合搜索**：支持向量 + 关键词

### 3.2.9 快速开始

```python
import weaviate

# 连接到 Weaviate
client = weaviate.Client("http://localhost:8080")

# 创建 Schema
class_obj = {
    "class": "Memory",
    "vectorizer": "text2vec-openai",
    "properties": [
        {"name": "content", "dataType": ["text"]},
        {"name": "memoryType", "dataType": ["string"]},
        {"name": "timestamp", "dataType": ["date"]}
    ]
}
client.schema.create_class(class_obj)

# 插入数据（自动向量化）
client.data_object.create(
    data_object={
        "content": "用户喜欢使用 Python 编程",
        "memoryType": "preference",
        "timestamp": "2024-01-15T10:00:00Z"
    },
    class_name="Memory"
)

# 语义搜索
result = client.query.get(
    "Memory", ["content", "memoryType"]
).with_near_text({
    "concepts": ["编程语言偏好"]
}).with_limit(5).do()
```

### 3.2.10 Docker 部署

```yaml
# docker-compose.yml
version: '3.4'
services:
  weaviate:
    image: semitechnologies/weaviate:latest
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-openai'
      ENABLE_MODULES: 'text2vec-openai'
      OPENAI_APIKEY: 'your-api-key'
```

### 3.2.11 适用场景

✅ 需要自托管、混合搜索、GraphQL 偏好
❌ 追求极致简单、完全无运维需求

---

### 3.2.12 Chroma

#### 特点

- **极简设计**：几行代码即可使用
- **内嵌模式**：无需独立服务
- **Python 原生**：与 Python 生态无缝集成
- **LangChain 集成**：开箱即用

### 3.2.13 快速开始

```python
import chromadb

# 创建客户端（内存模式）
client = chromadb.Client()

# 或持久化模式
client = chromadb.PersistentClient(path="./chroma_db")

# 创建集合
collection = client.create_collection(
    name="agent_memory",
    metadata={"hnsw:space": "cosine"}
)

# 插入数据（自动 Embedding）
collection.add(
    documents=[
        "用户偏好深色主题",
        "用户是 Python 开发者",
        "最近在学习机器学习"
    ],
    metadatas=[
        {"type": "preference"},
        {"type": "background"},
        {"type": "activity"}
    ],
    ids=["mem_1", "mem_2", "mem_3"]
)

# 查询
results = collection.query(
    query_texts=["用户的技术背景是什么？"],
    n_results=2
)
print(results['documents'])
# [['用户是 Python 开发者', '最近在学习机器学习']]
```

### 3.2.14 与 LangChain 集成

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)

# 直接用于 RAG
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
```

### 3.2.15 适用场景

✅ 快速原型、本地开发、小型项目、LangChain 用户
❌ 大规模生产、高并发场景

---

### 3.2.16 选型决策树

```
你的需求是什么？
│
├── 快速开发/原型验证 → Chroma
│
├── 生产环境，无运维能力 → Pinecone
│
├── 需要自托管 + 混合搜索 → Weaviate
│
├── 超大规模（亿级向量）→ Milvus
│
└── 中小规模 + 高性能 → Qdrant
```

### 3.2.17 性能优化建议

#### 选择合适的索引类型

| 索引类型 | 适用场景 | 查询速度 | 内存占用 |
|----------|----------|----------|----------|
| Flat | 小数据集 (<10K) | 最慢 | 最小 |
| IVF | 中等数据集 | 中等 | 中等 |
| HNSW | 大数据集，高性能需求 | 最快 | 较大 |

#### 向量维度选择

```python
# 常见 Embedding 模型维度
embedding_dimensions = {
    "text-embedding-ada-002": 1536,  # OpenAI
    "text-embedding-3-small": 1536,  # OpenAI (新)
    "text-embedding-3-large": 3072,  # OpenAI (新)
    "all-MiniLM-L6-v2": 384,         # Sentence Transformers
    "bge-large-zh": 1024,            # 中文优化
}

# 权衡：维度越高，语义表达越丰富，但存储和查询成本也越高
```

#### 元数据过滤

```python
# 使用元数据预过滤，减少向量搜索范围
results = collection.query(
    query_texts=["编程相关"],
    n_results=5,
    where={"type": "preference"}  # 先按元数据过滤
)
```

### 3.2.18 小结

向量数据库的选择应基于：
1. **规模**：数据量和查询并发量
2. **运维能力**：是否有专门的运维团队
3. **成本预算**：云服务 vs 自托管
4. **集成需求**：与现有技术栈的兼容性

对于大多数智能体项目，建议：
- **开发阶段**：使用 Chroma（简单快速）
- **生产阶段**：迁移到 Pinecone（托管）或 Weaviate（自托管）

下一节我们将探讨如何基于向量数据库构建高效的 RAG 系统。

---

**下一节**: [RAG 进阶：检索增强生成的最佳实践](3.3_rag_advanced.md)