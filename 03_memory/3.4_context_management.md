## 3.4 上下文窗口管理与压缩策略

大语言模型的上下文窗口是有限的资源。如何在有限的窗口中放入最有价值的信息，是构建高效智能体的关键挑战。

### 3.4.1 从 Prompt Engineering 到 Context Engineering

随着 Agent 系统的复杂度提升，单一的提示词工程（Prompt Engineering）已不足以应对挑战。我们需要升级到**上下文工程 (Context Engineering)** 的视角。

**Context Engineering** 是一门管理大语言模型有限"注意力预算 (Attention Budget)"的学科。这就好比计算机的 RAM 管理，不仅要关注"写入什么"（Prompt），更要关注"保留什么"（Context）。

核心挑战在于 **"Lost-in-the-Middle" (中段迷失)** 现象：当上下文过长时，模型往往容易忽略中间部分的信息，而更关注开头和结尾。因此，Context Engineering 的目标是：**用最少的高信噪比 tokens，换取最高的任务成功率。**

### 3.4.2 主流模型的上下文限制

| 模型 | 上下文窗口 | 大约字数（中文）|
|------|------------|-----------------|
| GPT-3.5-turbo | 16K tokens | ~8,000 字 |
| GPT-4-turbo | 128K tokens | ~64,000 字 |
| Claude 3.5 | 200K tokens | ~100,000 字 |
| Gemini 1.5 Pro | 1M tokens | ~500,000 字 |

### 3.4.3 上下文的构成

```
┌─────────────────────────────────────────────────────┐
│              上下文窗口 (Context Window)             │
│                                                     │
│  ┌─────────────────────────────────────────────┐   │
│  │ System Prompt (身份、规则、工具定义)          │   │
│  │ ~500-2000 tokens                             │   │
│  └─────────────────────────────────────────────┘   │
│                                                     │
│  ┌─────────────────────────────────────────────┐   │
│  │ 检索的知识/记忆                               │   │
│  │ ~1000-5000 tokens                            │   │
│  └─────────────────────────────────────────────┘   │
│                                                     │
│  ┌─────────────────────────────────────────────┐   │
│  │ 对话历史                                      │   │
│  │ ~2000-10000 tokens                           │   │
│  └─────────────────────────────────────────────┘   │
│                                                     │
│  ┌─────────────────────────────────────────────┐   │
│  │ 当前用户输入                                  │   │
│  │ ~100-1000 tokens                             │   │
│  └─────────────────────────────────────────────┘   │
│                                                     │
│  ┌─────────────────────────────────────────────┐   │
│  │ 预留给输出的空间                              │   │
│  │ ~1000-4000 tokens                            │   │
│  └─────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────┘
```

### 3.4.4 对话历史管理策略

#### 1. 滑动窗口

最简单的策略：保留最近的 N 条消息。

```python
class SlidingWindowMemory:
    def __init__(self, max_messages: int = 20):
        self.messages = []
        self.max_messages = max_messages
    
    def add(self, message: dict):
        self.messages.append(message)
        if len(self.messages) > self.max_messages:
            self.messages.pop(0)  # 移除最早的消息
    
    def get_context(self) -> List[dict]:
        return self.messages
```

**优点**：简单、低成本
**缺点**：可能丢失重要的早期上下文

#### 2. Token 预算管理

按 token 数量而非消息数量管理：

```python
import tiktoken

class TokenBudgetMemory:
    def __init__(self, max_tokens: int = 4000, model: str = "gpt-4"):
        self.messages = []
        self.max_tokens = max_tokens
        self.encoder = tiktoken.encoding_for_model(model)
    
    def count_tokens(self, messages: List[dict]) -> int:
        total = 0
        for msg in messages:
            total += len(self.encoder.encode(msg['content']))
            total += 4  # 消息格式开销
        return total
    
    def add(self, message: dict):
        self.messages.append(message)
        self._trim()
    
    def _trim(self):
        while self.count_tokens(self.messages) > self.max_tokens:
            # 保留 system 消息，移除最早的用户/助手消息
            for i, msg in enumerate(self.messages):
                if msg['role'] != 'system':
                    self.messages.pop(i)
                    break
```

#### 3. 重要性采样

保留重要的消息，移除不重要的：

```python
class ImportanceSamplingMemory:
    def __init__(self, max_tokens: int = 4000):
        self.messages = []
        self.importance_scores = []
        self.max_tokens = max_tokens
    
    def add(self, message: dict, importance: float = 0.5):
        """
        importance: 0.0-1.0，越高越重要
        - 1.0: 关键决策、用户明确要求记住的内容
        - 0.7: 重要的技术细节
        - 0.5: 普通对话
        - 0.3: 闲聊、确认性回复
        """
        self.messages.append(message)
        self.importance_scores.append(importance)
        self._trim()
    
    def _trim(self):
        while self._over_budget():
            # 找到重要性最低的非系统消息
            min_idx = -1
            min_score = float('inf')
            for i, (msg, score) in enumerate(zip(self.messages, self.importance_scores)):
                if msg['role'] != 'system' and score < min_score:
                    min_score = score
                    min_idx = i
            
            if min_idx >= 0:
                self.messages.pop(min_idx)
                self.importance_scores.pop(min_idx)
```

#### 4. 对话摘要压缩

定期将历史对话压缩为摘要：

```python
class SummaryMemory:
    def __init__(self, llm, summary_threshold: int = 10):
        self.llm = llm
        self.messages = []
        self.summary = None
        self.summary_threshold = summary_threshold
    
    def add(self, message: dict):
        self.messages.append(message)
        
        # 当消息过多时，压缩为摘要
        if len(self.messages) > self.summary_threshold:
            self._compress()
    
    def _compress(self):
        # 保留最近的几条消息
        recent = self.messages[-3:]
        to_summarize = self.messages[:-3]
        
        # 生成摘要
        summary_prompt = f"""请将以下对话历史压缩为简洁的摘要，保留关键信息：

{self._format_messages(to_summarize)}

摘要："""
        
        new_summary = self.llm.generate(summary_prompt)
        
        # 合并新旧摘要
        if self.summary:
            self.summary = f"{self.summary}\n{new_summary}"
        else:
            self.summary = new_summary
        
        self.messages = recent
    
    def get_context(self) -> List[dict]:
        context = []
        if self.summary:
            context.append({
                "role": "system",
                "content": f"[对话历史摘要]\n{self.summary}"
            })
        context.extend(self.messages)
        return context
```

### 3.4.5 内容压缩技术

#### 1. 提示词压缩

使用专门的压缩模型或算法：

```python
from llmlingua import PromptCompressor

compressor = PromptCompressor(model_name="microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank")

def compress_context(context: str, target_ratio: float = 0.5) -> str:
    """将上下文压缩到指定比例"""
    result = compressor.compress_prompt(
        context,
        rate=target_ratio,
        force_tokens=['重要', '关键'],  # 强制保留的词
    )
    return result['compressed_prompt']
```

#### 2. 选择性信息提取

只提取与当前任务相关的信息：

```python
def extract_relevant_info(history: List[dict], current_query: str) -> str:
    """从历史中提取与当前查询相关的信息"""
    
    prompt = f"""以下是对话历史：

{format_history(history)}

当前用户问题：{current_query}

请提取与当前问题相关的关键信息（如有的话）："""

    return llm.generate(prompt)
```

#### 3. 实体和事实提取

将对话转化为结构化知识：

```python
def extract_entities_and_facts(messages: List[dict]) -> dict:
    prompt = """从以下对话中提取结构化信息：

{conversation}

请按以下格式输出：
- 用户信息：[姓名、偏好、背景等]
- 讨论的主题：[主题列表]
- 达成的结论：[结论列表]
- 待办事项：[任务列表]
"""
    
    result = llm.generate(prompt)
    return parse_structured_info(result)
```

### 3.4.6 动态上下文管理：渐进式披露

在复杂的 Agent 系统中，一次性加载所有工具定义和技能说明极其浪费 Token。我们可以借鉴 UI 设计中的 **"渐进式披露" (Progressive Disclosure)** 理念：

> **原则**：Agent 初始状态只加载"技能目录"，只有当需要使用特定技能时，才加载该技能的详细上下文（Prompt、Examples）。

#### 实现模式：

1.  **Lightweight Registry**: 系统启动时，System Prompt 只包含技能名称和简短描述。
2.  **Lazy Loading**: 当 Router 决定调用某个技能时，动态注入该技能的详细指令。
3.  **Context Swapping**: 任务切换时，卸载旧技能的上下文，加载新技能的上下文。

这种类似"虚拟内存"的交换机制，可以让 Agent 在拥有海量技能的同时，保持轻量的上下文负载。

```mermaid
graph LR
    Start([任务开始]) --> Registry{检查技能目录}
    Registry -- 需要技能 A --> LoadA[动态加载技能 A\n(Prompt + Examples)]
    Registry -- 需要技能 B --> LoadB[动态加载技能 B\n(Prompt + Examples)]
    LoadA --> Execute[执行任务]
    LoadB --> Execute
    Execute --> Finish([任务结束])
    Execute -- 切换任务 --> Unload[卸载当前技能\n释放 Context]
    Unload --> Registry
    
    style Start fill:#f9f9f9,stroke:#333
    style Registry fill:#fff9c4,stroke:#fbc02d
    style LoadA fill:#e1f5fe,stroke:#0288d1
    style LoadB fill:#e1f5fe,stroke:#0288d1
    style Execute fill:#e8f5e9,stroke:#388e3c
    style Unload fill:#ffebee,stroke:#d32f2f
```

图 3-5：渐进式披露的工作流程

### 3.4.7 根据任务类型调整分配

```python
class DynamicContextManager:
    def __init__(self, total_budget: int = 8000):
        self.total_budget = total_budget
    
    def allocate(self, task_type: str) -> dict:
        """根据任务类型分配上下文预算"""
        
        allocations = {
            "code_generation": {
                "system": 500,
                "knowledge": 3000,  # 需要更多代码示例
                "history": 1000,
                "output": 3500
            },
            "conversation": {
                "system": 500,
                "knowledge": 1000,
                "history": 4000,   # 需要更多对话历史
                "output": 2500
            },
            "analysis": {
                "system": 500,
                "knowledge": 4000, # 需要更多背景资料
                "history": 1000,
                "output": 2500
            }
        }
        
        return allocations.get(task_type, allocations["conversation"])
```

### 3.4.8 优先级队列管理

```python
from heapq import heappush, heappop

class PriorityContextManager:
    def __init__(self, max_tokens: int = 4000):
        self.max_tokens = max_tokens
        self.items = []  # (priority, tokens, content)
    
    def add(self, content: str, priority: int, tokens: int):
        """priority 越大越重要"""
        heappush(self.items, (-priority, tokens, content))
    
    def build_context(self) -> str:
        """构建不超过预算的最优上下文"""
        selected = []
        current_tokens = 0
        
        # 按优先级取出
        temp = []
        while self.items:
            priority, tokens, content = heappop(self.items)
            temp.append((priority, tokens, content))
            
            if current_tokens + tokens <= self.max_tokens:
                selected.append(content)
                current_tokens += tokens
        
        # 恢复堆
        for item in temp:
            heappush(self.items, item)
        
        return "\n\n".join(selected)
```

### 3.4.9 实践建议

### 3.4.10 预算分配经验法则

| 组件 | 建议占比 | 说明 |
|------|----------|------|
| System Prompt | 5-10% | 精简但完整 |
| 检索知识 | 20-40% | 根据任务动态调整 |
| 对话历史 | 20-30% | 摘要 + 最近几轮 |
| 当前输入 | 5-15% | 通常用户输入不长 |
| 输出预留 | 20-30% | 确保模型有足够空间输出 |

### 3.4.11 监控和优化

```python
class ContextMonitor:
    def __init__(self):
        self.stats = []
    
    def log(self, context_tokens: int, output_tokens: int, task_type: str):
        self.stats.append({
            "context": context_tokens,
            "output": output_tokens,
            "task": task_type,
            "timestamp": datetime.now()
        })
    
    def analyze(self):
        """分析上下文使用模式"""
        avg_context = sum(s["context"] for s in self.stats) / len(self.stats)
        avg_output = sum(s["output"] for s in self.stats) / len(self.stats)
        
        print(f"平均上下文使用：{avg_context:.0f} tokens")
        print(f"平均输出长度：{avg_output:.0f} tokens")
        print(f"建议输出预留：{avg_output * 1.5:.0f} tokens")
```

### 3.4.12 小结

上下文窗口管理的核心原则：

1. **预算意识**：始终清楚每个组件消耗多少 tokens
2. **动态调整**：根据任务类型灵活分配资源
3. **压缩优先**：能压缩的信息尽量压缩
4. **保留核心**：关键信息永远不能被丢弃
5. **持续监控**：了解实际使用模式，持续优化

掌握上下文管理，是构建高效、经济的智能体系统的必备技能。

下一节我们将进入第四章，探讨智能体的工具使用与环境交互能力。

---

**下一节**: [本章小结](summary.md)