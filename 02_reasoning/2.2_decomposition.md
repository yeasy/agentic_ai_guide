## 2.2 任务分解算法：ToT 与 GoT

### 2.2.1 规划：智能体的前额叶皮层

在人类大脑中，前额叶皮层负责高级认知功能，如规划、决策和执行控制。而在 Agent 架构中，**规划 (Planning)** 模块扮演着同样的角色。面对简单的问候，Agent 可以依靠直觉（LLM 的训练权重）直接回答；但面对“开发一个貪吃蛇游戏并部署到 Vercel”这样的复杂目标，没有规划的 Agent 就像无头苍蝇，走一步看一步，最终迷失在细节中。

规划的核心目标是**将抽象的、复杂的长期目标（Goal）转化为具体的、可执行的原子行动序列（Plan）**。

---

### 2.2.2 任务分解 (Task Decomposition)

这是规划的第一步。Agent 需要利用 LLM 的逻辑推理能力，将大任务拆解为小任务。

#### Chain of Thought (CoT) 与思维链
最基础的拆解技术。通过 `Let's think step by step` 的提示，诱导模型将推理过程显式化。
*   **Zero-shot CoT**：直接告诉模型这一句咒语。
*   **Few-shot CoT**：提供几个“问题 -> [思考过程] -> 答案”的示例，让模型模仿这种思考模式。

#### Tree of Thoughts (ToT) 与思维树
对于创意写作或复杂解谜，线性的 CoT 往往不够。ToT 允许 Agent 在思维的每一步探索多个分支。
*   **机制**：Agent 生成 3 个可能的下一步计划（Node A, Node B, Node C）。
*   **评估**：Agent 自我评估这 3 个计划的可行性（State Evaluator）。
*   **搜索**：使用 BFS（广度优先）或 DFS（深度优先）搜索最优路径。如果发现 Node A 走不通，回溯并尝试 Node B。

#### LLM+P (External Planners)
对于极度依赖逻辑的场景（如机器人搬箱子），LLM 可能不如传统的符号规划器（Symbolic Planner）准确。LLM+P 方法将自然语言问题转化为 **PDDL (Planning Domain Definition Language)**，交给外部的经典规划求解器（Solvers）算出最优步数，再翻译回自然语言。

---

### 2.2.3 主流规划模式详解

#### ReAct (Reasoning and Acting)
这是 Yao et al. 在 2022 年提出的里程碑式论文。它的核心思想是**行动与推理的交织**。
*   **传统模式**：要么只推理不行动（CoT），要么只行动不推理。
*   **ReAct 循环**：
    1.  **Thought**：用户想查某人的年龄，我应该先去搜维基百科。
    2.  **Action**：`Search_Wikipedia("Barack Obama")`
    3.  **Observation**：搜索结果返回了 Obama 的生平。
    4.  **Thought**：我找到了出生日期，现在计算年龄。
    5.  **Action**：`Calculator(2025 - 1961)`
    6.  **Observation**：64。
    7.  **Answer**：他 64 岁了。
*   **优点**：鲁棒性强，每一步都能根据最新的观察调整策略。
*   **缺点**：Token 消耗大，且容易陷入局部最优或死循环。

#### Plan-and-Solve (规划与执行分离)
对于步骤清晰的长任务，ReAct 显得太唠叨。Plan-and-Solve 模式将过程分为两个阶段：
1.  **Planner Agent**：不执行任何操作，只负责列出详细的 1-2-3-4 步骤清单。
2.  **Executor Agent**：拿着清单，逐条执行。
*   **变体 (Replanning)**：Executor 如果在第 3 步执行失败，会停下来，把当前情况反馈给 Planner，要求 Planner 基于现状更新剩余的计划（Steps 3-5）。

---

#### 反思与自我修正 (Self-Reflection / Reflexion)

没有完美的计划。高级 Agent 必须具备**元认知 (Metacognition)** 能力，即“思考自己的思考”。

#### Reflexion 框架
这是一个通过语言反馈强化 Agent 的框架。
*   **触发**：当 Agent 的回答被验证器（Verifier）驳回，或者代码运行报错时。
*   **反思**：Agent 不会通过梯度下降更新权重，而是更新**短期记忆**。它会生成一段文本：“我上次失败是因为没有导入 `numpy` 库，这次我必须检查导入。”
*   **效果**：这种“口头教训”被加入到下一次尝试的 Context 中，显著提升成功率。研究表明，Reflexion 能让 GPT-4 在 HumanEval 编程测试中的分数大幅提升。

### 2.2.4 本节小结
规划模块决定了 Agent 的上限。从简单的线性 CoT 到复杂的 ToT 搜索，再到具备自我修复能力的 Reflexion 框架，我们正在一步步逼近人类解决问题的核心思维模式。未来的规划器将更加模块化，甚至可能出现专门负责“挑刺”的 Critic Agent，与负责“干活”的 Actor Agent 形成对抗进化。

---

**下一节**: [ReAct：推理与行动的统一](2.3_react.md)