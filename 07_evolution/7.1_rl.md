## 7.1 从反馈中学习：RLHF 与 RLAIF

**预训练**教会了 LLM "预测下一个词"，但这并不意味着它学会了"如何做一个好的智能体"。要让智能体的行为符合人类的意图（**有用、诚实、无害**，即 3H 原则），需要引入强化学习机制。

本节深入探讨 **基于人类反馈的强化学习（RLHF）**及其变体**基于 AI 反馈的强化学习（RLAIF）**，展示如何通过环境反馈来优化智能体的决策策略。

### 7.1.1 为什么智能体需要强化学习

智能体的目标不是生成“看起来合理”的文本，而是在环境中完成任务并长期保持稳定行为。强化学习的价值在于把“偏好/目标”转化为可优化的信号：通过奖励函数、比较反馈或过程监督，让策略在反复试错中朝着期望方向收敛。

### 7.1.2 模仿学习的局限

**基座模型**主要通过 **监督微调** 获得能力。给模型看 1000 个"正确行为"的例子，让它模仿。

但在智能体场景下，模仿学习面临挑战：

| 问题 | 描述 | 例子 |
|------|------|------|
| **多步推理** | 只有最终结果有标签，中间步骤很难标注 | "证明费马大定理" |
| **探索性** | 可能存在比训练数据更好的解法 | "寻找更短的排序代码" |
| **分布偏移** | 部署环境与训练数据分布不同 | 遇到的新 API 错误 |

#### 强化学习的优势

强化学习（RL）通过 **奖励**而非**正确答案** 来指导学习。

- **SFT**: "你应该在这里输出单词 'Hello'。"
- **RL**: "你输出了什么我不关心，只要用户最后点了赞，你就赢了。"

对于智能体，RL 允许模型在试错中探索出最优的工具使用策略和规划路径。

### 7.1.3 RLHF 核心流程

RLHF 通常分为三个阶段：

1. **有监督微调 (SFT)**
    - 收集高质量的 (Prompt, Response) 对。
    - 目的：让模型先学会"像人一样说话"，进入状态。
    
2. **奖励模型训练 (RM)**
    - 收集比较数据：同一 Prompt，模型生成 A 和 B。
    - 人类标注员选择更好的一个（A > B）。
    - 训练一个 **奖励模型**，输入 (提示词 Prompt, 响应 Response)，输出一个标量分数。
    
    ```python
    def reward_model(prompt, response) -> float:
        # 这是一个被训练过的神经网络

        score = model(prompt + response)
        return score
    ```

3. **近端策略优化 (PPO)**
    - 使用强化学习算法（如 **近端策略优化（PPO）**）微调语言模型。
    - 目标：最大化 RM 给出的分数，同时防止模型偏离初始模型太远（KL 散度约束）。

### 7.1.4 在智能体中的应用

在智能体领域，RLHF 不仅用于优化对话，更用于优化 **行动**。

- **浏览器/工具型智能体训练**：通过对“答案质量”与“工具使用轨迹”（查询、点击、调用参数等）同时打分，让策略学会更可靠的外部信息获取与执行。
- **Tool Use**: 当智能体成功调用工具并解决问题时给予正反馈；当智能体产生幻觉参数时给予负反馈。

### 7.1.5 RLAIF：用 AI 训练 AI

#### 人类反馈的瓶颈

RLHF 效果虽好，但面临两大瓶颈：
1. **昂贵**：高质量的人类标注非常贵。
2. **缓慢**：无法实时反馈。
3. **能力上限**：对于超复杂的任务（如编写操作系统内核），普通标注员无法判断对错。

#### 宪法 AI：Constitutional AI

**宪法 AI（Constitutional AI）** 思路是用“原则集合 + 评审器”替代部分人工反馈：让另一个模型根据原则对回答进行比较与批改。

**基本流程**：

1. **制定宪法 (Constitution)**：
   一组自然语言原则。例如："请选择那个更乐于助人、更诚实且无害的回答。"

2. **AI 反馈 (AI Feedback)**：
   让一个评审模型扮演裁判，根据宪法给弱模型（或它自己）生成的回答打分。

   ```python
   prompt = """
   作为一名基于以下原则行事的 AI 助手：
   {constitution}
   
   请评估以下两个回答中哪一个更好：
   回答 A: {response_a}
   回答 B: {response_b}
   """
   ```

3. **RL 训练**：
   使用 AI 生成的偏好数据训练 Reward Model，后续流程与 RLHF 相同。

**优势**：
- **可扩展性**：AI 可以 24/7 工作，成本极低。
- **透明性**：只需调整"宪法"文本，即可改变模型的行为准则。

### 7.1.6 过程监督：结果奖励与过程奖励

在传统的 RLHF 中，我们通常使用：
- **结果奖励**：仅对任务的最终成败进行评价。
  - 问题：**稀疏奖励**。智能体走了 50 步，最后一步错了导致失败，模型不知道前 49 步其实是对的。

- **过程奖励**：对每一步推理及行动进行打分。
  - 优势：**密集奖励**。能更精准地指导模型。

### 7.1.7 过程奖励模型

在数学推理等任务中，过程奖励模型（PRM）常被用来提升多步推理的稳定性：相比只在最终结果打分，它能更早发现错误并指导剪枝。**实现方式**：
1. 让模型生成思维链（CoT）。
2. 人类（或 AI）标注每一步推理是"正确"、"错误"还是"中性"。
3. 训练一个 PRM，能够给推理的中间步骤打分。

```python
# PRM 的应用：Best-of-N 采样

def generate_best_solution(problem):
    solutions = []
    for _ in range(N):
        # 逐步生成

        steps = []
        current_context = problem
        
        while not done:
            step = generate_step(current_context)
            score = prm_model(current_context + step)
            
            # 如果某一步分数太低，直接剪枝

            if score < THRESHOLD:
                break
                
            steps.append(step)
            current_context += step
            
        if done:
            solutions.append((steps, final_score))
            
    return max(solutions, key=lambda x: x[1])
```

### 7.1.8 直接偏好优化

#### PPO 的复杂性

PPO 需要训练一个 Reward Model，还需要在训练中加载四个模型（Policy, Reference, Reward, Value），显存消耗极大，且极其不稳定。

#### DPO

斯坦福团队提出的 **直接偏好优化（DPO）** 证明了：可以跳过 Reward Model 训练阶段，直接在偏好数据上优化语言模型。

$$ L_{DPO} = -E[\log(\sigma(\beta(\log(p_w/p_{w,ref}) - \log(p_l/p_{l,ref}))))] $$

- **$y_w$**: 胜出的回答 (Winner)
- **$y_l$**: 失败的回答 (Loser)
- **$p_w$ / $p_l$**: 当前策略 $\pi_\theta$ 对 $y_w$ / $y_l$ 的概率
- **$p_{w,ref}$ / $p_{l,ref}$**: 参考策略 $\pi_{ref}$ 对 $y_w$ / $y_l$ 的概率

**DPO 的意义**：它将强化学习问题转化为了一个简单的分类损失函数优化问题。这大大降低了智能体微调的门槛，使得个人开发者也能在消费级显卡上微调出符合特定偏好的智能体。

### 7.1.9 组内相对优化（GRPO 思路）

在推理模型训练中，一类常见思路是用“组内相对比较”来优化策略：对同一个提示采样多条候选输出，在组内比较高低优劣，并据此更新策略。

这种做法的工程价值通常在于：

- **降低额外模型开销**：减少对独立价值网络（Critic/Value Model）的依赖或其规模。
- **训练更稳健**：相对比较往往能降低方差，减少训练抖动。
- **促进自检行为**：在合适的奖励设计下，模型更容易学会验证、反思、回溯等策略。

### 7.1.10 小结
  
  强化学习是智能体从"可用"走向"好用"的关键技术。

- **RLHF** 让智能体对齐人类价值观。
- **RLAIF** 解决了规模化扩展的难题。
- **Process Supervision** 提升了复杂推理的可靠性。
- **DPO** 降低了算法落地的门槛。

未来的智能体训练将更多地依赖 **环境反馈**（例如编译器报错、API 调用结果）而非单纯的人类标注，形成自我进化的数据飞轮。

下一节将探讨如何科学地评估智能体的能力。

---

**下一节**: [7.2 评估体系：AgentBench 与基准测试](7.2_evaluation.md)
