## 7.2 评估体系：AgentBench 与基准测试

在传统的 NLP 任务中，我们有 BLEU、ROUGE 甚至 Accuracy 这样清晰的指标。但在 Agent 领域，评估（Evaluation）变得异常困难：怎么判断一个 Agent "做对了"？怎么量化它的"智能"？

本节将深入探讨 Agent 评估的独特挑战，介绍构建立体评估体系的方法，并解析目前主流的基准测试框架。

#### Agent 评估的独特挑战

#### 解法的非唯一性
要达成"订一张去上海的票"这个目标，Agent 可以先查航班再订票，也可以先查高铁，甚至可以发邮件给秘书。路径千变万化，无法用标准答案（Gold Standard）进行字符串匹配。

#### 环境副作用
Agent 的操作会改变世界状态。
- **测试前**：数据库里没有订单。
- **测试后**：数据库里多了一条订单。
评估必须在一个隔离的**沙箱环境**中进行，并且支持**状态重置**。

#### 多维度权衡
单个指标无法概括 Agent 的表现：
- **成功率 (Success Rate)**：任务完成了吗？
- **效率 (Efficiency)**：用了多少步？花了多少 Token？耗时多久？
- **安全性 (Safety)**：有没有泄露隐私？有没有误删文件？
- **鲁棒性 (Robustness)**：Prompt 稍微变一点，还能完成吗？

#### 评估方法论体系

建立一整套评估体系，通常包含三个层次：

### 7.2.1 Level 1: 单元测试

针对特定工具或子任务的确定性测试。

- **适用对象**：Tool Selection, Function Calling 格式。
- **方法**：
  - 给定输入："查询北京天气"。
  - 断言输出：必须包含 `get_weather(city='Beijing')` 调用。
- **工具**：`pytest`, `unittest`。

```python
def test_calculator_tool():
    agent = Agent(tools=[Calculator()])
    response = agent.run("23 * 4等于多少？")
    
    # 检查是否正确调用了工具
    assert "calculator" in agent.trace.tool_calls
    # 检查最终结果
    assert "92" in response.text
```

#### Level 2: 轨迹评估

评估 Agent 解决问题的过程质量。这通常需要 LLM-as-a-Judge（让 LLM 当裁判）。

- **方法**：记录 Agent 的完整执行轨迹（Trajectory），喂给 GPT-4 进行打分。
- **维度**：
  - **逻辑性**：推理步骤是否连贯？
  - **幻觉**：是否编造了不存在的参数？
  - **冗余**：有没有重复调用同一个工具？

```python
EVAL_PROMPT = """
请评估以下 Agent 执行轨迹的质量：

任务：{task}
轨迹：
{trajectory}

请从以下维度打分 (1-5)：
1. 目标达成度
2. 工具使用效率
3. 推理逻辑性
"""
```

#### 进阶模式：LLM-as-Judge 最佳实践

单纯的让 LLM 打分容易产生偏差（Bias），以下是三种更稳健的评估模式：

**1. Direct Scoring (直接评分法)**
使用带有权重和详细标准的量规 (Rubric)，而非模糊的 "1-5 分"。
*   *关键点*：明确定义每个分数的含义（例如：3分=完成任务但有冗余，4分=完成任务且高效）。

**2. Pairwise Comparison (成对比较法)**
类似于 RLHF 中的 Reward Model 训练，让 LLM 对比两个不同的轨迹 A 和 B，选出更好的一个。
*   *优势*：避免了绝对分数的方差，人类/模型在做"二选一"时往往比"打分"更准确。
*   *注意*：需要交换位置（A vs B 和 B vs A）运行两次以消除**位置偏差 (Position Bias)**。

**3. Rubric Generation (自动化量规生成)**
在特定垂直领域，手写评估标准很累。可以让强大的模型（如 GPT-4）先阅读标准操作程序 (SOP)，然后生成该领域的评估标准。

```mermaid
graph LR
    Input([测试指令]) --> GenA[Agent A 生成轨迹]
    Input --> GenB[Agent B 生成轨迹]
    GenA & GenB --> Judge{LLM 裁判}
    Judge --> WinA[A 胜]
    Judge --> WinB[B 胜]
    Judge --> Tie[平局]
    
    style Input fill:#f9f9f9,stroke:#333
    style GenA fill:#e3f2fd,stroke:#1565c0
    style GenB fill:#e3f2fd,stroke:#1565c0
    style Judge fill:#fff9c4,stroke:#fbc02d
```

图 7-1：成对比较法（Pairwise Comparison）评估流程


### 7.2.2 Level 3: 端到端基准测试

在真实或模拟环境中运行完整任务。

- **成功判定**：通过检查环境状态变化来判定。
- **例如**：任务是"在当前目录下创建一个名为 test.txt 的文件"。评估脚本运行后检查 `os.path.exists('test.txt')` 是否为 True。

### 7.2.3 主流基准测试框架

#### AgentBench (清华大学)

[AgentBench](https://github.com/THUDM/AgentBench) 是目前最全面的综合评估框架之一，涵盖 8 个核心能力维度：

| 环境 | 描述 | 测试能力 |
|------|------|----------|
| **OS World** | 真实的 Linux Bash 环境 | 操作系统指令、文件管理 |
| **Database** | SQL 数据库交互 | 数据库查询与操作 |
| **Knowledge Graph** | 知识图谱游走 | 复杂知识推理 |
| **AlfWorld** | 虚拟家居环境 | 具身智能、规划 |
| **WebShop** | 模拟电商网站 | 网页浏览、决策 |
| **Card Game** | 数字卡牌游戏 | 博弈论、对抗策略 |

**特点**：全方位、跨领域，不仅测对话，更测行动。

#### GAIA

由 Meta, HuggingFace, AutoGPT 联合推出。[GAIA](https://arxiv.org/abs/2311.12983) 的特点是**问题极其困难**且**贴近真实生活**。

- **问题示例**：
  > "请找到这篇 PDF 论文中提到的所有数据集，并对比它们在 2022 年的引用量变化趋势，最后生成一张折线图。"
  
- **难度**：这需要 Agent 结合 PDF 读取、Web 搜索、数据提取、Python 代码编写和图表绘制多种能力。
- **现状**：GPT-4 在这里的得分通常也不到 40%，通过率极低，区分度极高。

#### SWE-bench (软件工程)

专注于编程和软件工程能力的评估。

- **数据源**：从真实的 GitHub 开源项目中收集的 Issue 和 Pull Request。
- **任务**：给定一个庞大的代码库和一个 Bug 描述，Agent 需要：
  1. 定位 Bug 代码。
  2. 编写复现 Bug 的测试用例。
  3. 修改代码修复 Bug。
  4. 确保通过所有测试。
- **意义**：这是目前 Coding Agent 领域的"黄金标准"。

#### WebArena (网页浏览)

专门评估 Agent 操控浏览器的能力。

- **环境**：完全模拟的电商、论坛、代码托管网站。
- **任务**："在这个网站上买最便宜的红色雨伞，并寄到这个地址..."
- **评估**：检查最终的页面状态和数据库记录。

#### Terminal-Bench (Linux 终端)
专注于评估 Agent 在 Linux 命令行环境下的任务执行能力。
- **任务**：文件操作、系统管理、网络配置等。
- **意义**：评估 Agent 作为系统管理员或 DevOps 的潜力。

#### Tau-Bench (对话交互)
专注于评估 Conversational Agent 的多轮对话与工具使用能力。
- **特点**：模拟真实用户（User persona）与 Agent 进行多轮交互，测试 Agent 是否能在保持对话连贯的同时完成复杂任务（如改签机票、处理退款）。

#### MAST-Data：多智能体系统失败轨迹数据集

[MAST-Data](https://arxiv.org/abs/2503.13657) 是首个专门针对多智能体系统失败模式的标注数据集，包含 1642 条执行轨迹。

| 维度 | 覆盖范围 |
|------|---------|
| **框架** | MetaGPT, ChatDev, AG2, AppWorld, HyperAgent, OpenManus, ADAS |
| **模型** | GPT-4 系列, Claude 系列, Qwen2.5, CodeLlama |
| **任务** | 编程、数学推理、通用 Agent 任务 |

**独特价值**：与其他 Benchmark 不同，MAST-Data 不仅记录成功/失败结果，还标注了失败的具体原因类型。这使得开发者可以：

1. **定向优化**：识别系统中最频繁的失败模式
2. **对比分析**：比较不同架构在相同任务上的失败分布
3. **验证改进**：量化地验证修复措施的效果

**任务验证问题（FC3）**：MAST 研究特别关注了验证相关的失败：
- **FM-3.1 过早终止**：Agent 在任务未完成时停止
- **FM-3.2 验证不完整**：缺少必要的结果检查
- **FM-3.3 验证错误**：验证逻辑本身存在缺陷

#### 评估与模型选型指南

在为你的 Agent 系统选择基座模型（Base Model）时，不要只看 MMLU 分数（那个主要是测做题能力的）。

**选型建议**：

1. **Coding 任务**：首选 **SWE-bench** 分数高的模型（如 Claude 4 Sonnet, GPT-5）。
2. **工具调用**：关注 **Berkeley Function Calling Leaderboard**。
3. **长上下文**：关注 **Needle In A Haystack (大海捞针)** 测试。

### 7.2.4 进阶评估策略

随着 Agent 应用的深入，仅靠简单的通过率已经无法满足需求。我们需要更精细的指标和更立体的防御体系。

#### 1. 瑞士奶酪模型 (The Swiss Cheese Model)

在安全工程中，没有单一的防御层是完美的。Agent 评估也是如此，我们需要建立多层防御体系，让一层漏掉的缺陷被下一层捕捉。

- **Layer 1: 自动化评估 (Automated Evals)**
  - **特点**：快速、低成本、高频运行（每次 Commit）。
  - **内容**：单元测试、基于 LLM 的评分（Promptfoo 等工具）。
  - **作用**：即使没有真实用户，也能大规模测试基准任务，建立信心。

- **Layer 2: 生产环境监控 (Production Monitoring)**
  - **特点**：真实世界的 Ground Truth。
  - **内容**：监控错误率、延迟、用户投诉、Token 消耗。
  - **作用**：发现自动化测试覆盖不到的 "长尾问题" 和 "分布漂移" (Distribution Drift)。

- **Layer 3: A/B 测试**
  - **特点**：统计学显著性。
  - **内容**：对比新旧版本的转化率、任务完成率。
  - **作用**：在全面上线前验证改动的实际业务价值。

- **Layer 4: 人工审查 (Manual Review)**
  - **特点**：慢、贵，但是黄金标准。
  - **作用**：定期抽样阅读轨迹日志（Transcript），通过人工直觉发现隐蔽的质量问题，并用于校准（Calibrate）自动化评估的 LLM 裁判。

#### 2. 非确定性与概率性指标

Agent 的行为本质上是非确定性（Non-deterministic）的。同一个任务跑两次，结果可能不同。因此，我们需要概率性指标：

- **pass@k**: 在 k 次尝试中，**至少有一次**成功的概率。
  - **适用场景**：创意生成、代码生成（只要生成一个能跑的方案就行）。
  - *意义*：衡量 Agent 的"上限"潜力。

- **pass^k**: 在 k 次尝试中，**每一次都**成功的概率。
  - **计算**：如果单次成功率是 p，k 次全对的概率约等于 $p^k$。
  - **适用场景**：面向客户的客服、金融操作（必须极其稳定，不能时好时坏）。
  - *意义*：衡量 Agent 的"下限"可靠性。

#### 3. 能力评估 vs 回归评估

- **能力评估 (Capability Evals)**：针对 Agent 目前还做不到或勉强能做到的难题，用来"爬坡"。通过率通常较低。
- **回归评估 (Regression Evals)**：针对 Agent 已经掌握的技能，用来"守底"。通过率应接近 100%。防止新模型或新提示词导致原有能力退化。

### 7.2.5 小结

评估是 Agent 开发中最容易被忽视，但最重要的环节。"如果你无法度量它，你就无法改进它。"

构建一个可靠的 Agent 系统，你需要：
1. **CI/CD 中的单元测试**：保证工具调用的基本逻辑不坏。
2. **定期的 LLM 评分**：监控推理质量的变化。
3. **引入公开 Benchmark**：明确你的 Agent 在行业中的水平。

下一节我们将探讨如何通过轨迹分析来提升 Agent 的可解释性。

---

**下一节**: [轨迹分析与行为可解释性](7.3_tracing.md)