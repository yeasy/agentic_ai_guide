## 12.4 案例模板与检查清单

本节用于解决“如何把智能体方案写成可复用、可验收、可迭代的交付案例”。

---

### 12.4.1 标准案例呈现模板

建议所有实战案例（Case Study）统一采用如下结构。

#### 案例背景

- **行业/场景**：说明具体业务环境（如金融风控、电商客服、软件开发）。
- **用户痛点**：说明引入智能体前的核心问题（如人工流程慢、规则维护成本高、跨系统协作困难）。

#### 目标设定

- **核心目标**：一句话描述要解决的问题。
- **关键指标（KPI/SLO）**：给出可量化目标，例如：
  - 平均响应时间下降 30%
  - 任务成功率提升到 90%+
  - 人工介入率下降 40%

#### 解决方案

- **架构设计**：说明采用的智能体架构（单体/多智能体、ReAct/ToT/分层等）。
- **关键技术栈**：
  - **模型选型**：基于能力、成本、延迟、合规等因素说明选择理由。
  - **记忆设计**：短期缓存、检索增强、结构化存储如何配合。
  - **工具体系**：数据库、搜索、代码执行、外部 API 等工具如何接入与授权。
- **工作流图示**：建议使用 Mermaid 展示关键路径。

```mermaid
flowchart LR
    classDef agent fill:#e6f7ff,stroke:#1890ff,stroke-width:2px;
    classDef tool fill:#f6ffed,stroke:#52c41a,stroke-width:2px;
    classDef user fill:#fff7e6,stroke:#fa8c16,stroke-width:2px;
    classDef memory fill:#fff0f6,stroke:#eb2f96,stroke-width:2px;

    Planner[[Planner Agent]] -->|任务拆解| Dev[Developer Agent]
    Planner --> QA[QA Agent]
    Dev --> Repo[(代码仓库)]
    QA --> Repo
    QA --> Human[(HITL 审批)]

    class Planner agent;
    class Dev,QA agent;
    class Repo memory;
    class Human user;
```

图 12-4：标准协作架构图模板

#### 实施成效

- **定性结果**：哪些流程被改善、用户体验如何变化。
- **定量结果**：对照目标前后数据，例如延迟、错误率、吞吐、成本变化。

#### 风险与教训

- **主要风险**：如幻觉、工具失败、长链路延迟、越权调用、成本超预算。
- **经验沉淀**：哪些做法有效、哪些应避免、下一轮优化优先级是什么。

---

### 12.4.2 实施与验收检查清单

以下清单用于评估案例是否达到“可复现、可运行、可验收”标准。

#### A. 工程实现检查（必选）

- [ ] **输入输出定义**：是否给出输入数据结构与输出结构（建议 JSON Schema）。
- [ ] **依赖与环境**：是否列出库版本、环境变量、外部服务与权限要求。
- [ ] **核心实现**：是否提供关键流程伪代码或核心代码片段。
- [ ] **错误处理**：是否覆盖超时、拒答、工具失败、重试与降级策略。
- [ ] **可观测性**：是否说明日志、追踪、评估集或告警配置。
- [ ] **运行说明**：是否给出可执行的启动、测试、回归验证命令。

#### B. 工具开发检查（按需）

- [ ] **幂等性**：重复调用是否安全；写操作重复执行是否可控。
- [ ] **参数校验**：是否校验类型、取值范围、必填项与默认值。
- [ ] **安全边界**：是否防止路径遍历、注入、越权访问等风险。
- [ ] **高风险操作审查**：是否为敏感写操作设置 Human-in-the-loop。
- [ ] **容错机制**：是否具备超时、重试、熔断或回退策略。
- [ ] **可理解报错**：返回信息是否便于智能体进行下一步自修复。

---

### 12.4.3 版本升级前的更新检测流程

在升级模型、SDK、工具链前，建议执行以下流程：

1. **能力变更检查**：确认结构化输出、工具调用、上下文窗口、缓存机制是否变化。
2. **破坏性变更检查**：确认依赖是否有主版本升级、参数弃用或默认行为变更。
3. **回归验证**：运行评估集与关键链路回放，比较成功率、延迟、成本与安全指标。
4. **灰度发布与回滚预案**：先小流量验证，再全量；保留一键回滚路径。

---

### 12.4.4 AgentOps 指标清单（参考）

指标阈值不应“一刀切”，应由业务 SLA/SLO 定义。下表给出常见观察维度：

| 指标类别 | 具体指标 | 设定方式 | 备注 |
| :--- | :--- | :--- | :--- |
| **效果** | 任务成功率、步骤准确率 | 按业务目标设定 | 核心链路设更高阈值 |
| **性能** | 端到端延迟、首字生成时间 | 按交互体验设定 | 离线任务可放宽 |
| **成本** | 单任务词元、日均调用成本 | 按预算与 ROI 设定 | 监控异常波动 |
| **稳定性** | 工具失败率、重试成功率 | 按可用性目标设定 | 需配合告警与降级 |
| **安全** | 护栏触发率、越权调用率 | 按风控要求设定 | 高风险场景需人工审计 |

---

### 12.4.5 使用建议

- 先用 **12.4.1 模板**写清“问题—方案—结果—风险”。
- 再用 **12.4.2 清单**做交付前自检，确保可运行与可验收。
- 最后用 **12.4.3 + 12.4.4**建立持续迭代与监控机制。

当案例能被新成员在最短时间内复现，并在升级后稳定运行时，模板才真正发挥价值。

---

### 12.4.6 最小可复现实验标准定义

在智能体开发与论文/案例发布中，为了确保评估的公正性与他人的可验证性，必须提供**最小可复现实验 (Minimal Reproducible Experiment, MRE)**。一个合格的 MRE 必须以代码仓库或沙箱镜像的形式交付，并包含以下四个硬性标准：

1. **固定的基座模型与参数**：
   - 必须锁死模型版本（如 `gpt-4o-2024-05-13`）
   - 公开 `temperature`、`top_p` 等生成超参数
   - 禁止使用随时波动或会被服务商静默替换的 `latest` 标签
2. **确定性的测试集与静态评估脚本**：
   - 包含足够数量（建议至少 50 个以上）带有明确 Ground Truth 的测试用例数据文件（如 JSONL 格式）
   - 配平了指标权重的离线自动化评估脚本
3. **隔离的沙箱环境与 Mock 依赖**：
   - 所有的外部执行环境（如数据库快照、依赖服务、终端隔离系统）必须通过 Docker Compose 或 DevContainer 提供一键隔离启动配置
   - 切断对极易发生变更的不可控公网 API 服务的直接依赖
4. **透明的提示词与全链路轨迹**：
   - 不仅要公开完整的系统提示词（System Prompt）与工具 Schema 定义
   - 还应在系统中提供包含 `trace_id` 的日志回放工具
   - 并附带至少 3-5 份成功与失败的真实长上下文 Trace 日志片段用于调试对照

---

**上一节**: [项目说明文件规范指南](12.3_agents_md.md)

**返回主页**: [返回目录](../SUMMARY.md)
