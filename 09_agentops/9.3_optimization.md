## 9.3 性能优化与成本控制

智能体系统的运营成本主要来自 LLM API 调用。一个设计不当的智能体可能每个任务消耗数万 Token，导致成本失控。本节探讨如何在保证质量的前提下优化性能和控制成本。

### 9.3.1 成本构成分析

### 9.3.2 LLM 调用成本

以 GPT-4 为例分析成本构成：

| 模型 | 输入价格 ($/1M tokens) | 输出价格 ($/1M tokens) |
|------|------------------------|------------------------|
| GPT-4 Turbo | $10 | $30 |
| GPT-4o (Legacy) | $2 | $5 |
| GPT-5 Preview | $15 | $45 |
| Claude 4 Opus | $15 | $60 |
| Claude 4 Sonnet | $3 | $12 |

**典型智能体任务的 Token 消耗**：

```python
# 示例：一次 ReAct 循环的词元 (Token) 估算
tokens_per_step = {
    "system_prompt": 500,
    "user_query": 100,
    "history": 1000,  # 累积
    "tool_description": 300,
    "thought_output": 200,
    "tool_result": 500,
}
# 假设 5 轮循环
total_input = (500 + 100 + 300) * 5 + 1000 * 5  # ~9500 tokens
total_output = 200 * 5 + 500  # ~1500 tokens
```

### 9.3.3 隐藏成本

除了直接的 API 费用，还有：

- 向量数据库存储和查询
- 嵌入模型调用
- 缓存服务器
- 监控和日志

### 9.3.4 Token 优化策略

#### 提示词压缩 (Prompt Compression)

减少 System Prompt 的冗余：

```python
# ❌ 冗余的 prompt
system_prompt = """
你好！你是一个非常友好和乐于助人的 AI 助手。你的主要职责是帮助用户
解决各种问题。你应该始终保持礼貌和专业。当用户提出问题时，你需要
仔细分析问题，考虑各种可能的解决方案，然后给出最佳答案。如果你
不确定答案，你应该诚实地告诉用户你不知道...
"""  # ~150 tokens

# ✅ 精简的 prompt
system_prompt = """
角色：专业 AI 助手
规则：1) 准确回答 2) 不确定时说明 3) 使用工具获取信息
"""  # ~30 tokens
```

**压缩技巧**：

- 使用项目符号而非长段落
- 删除礼貌用语（LLM 已经很礼貌）
- 使用缩写和符号
- 只保留必要的规则

#### 上下文窗口管理

避免无限累积历史消息：

```python
class ContextManager:
    def __init__(self, max_tokens: int = 4000):
        self.max_tokens = max_tokens
        self.messages = []
        
    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})
        self._trim_context()
        
    def _trim_context(self):
        """保留最重要的上下文"""
        total_tokens = self._count_tokens()
        
        while total_tokens > self.max_tokens and len(self.messages) > 2:
            # 保留第一条（通常是重要背景）和最近几条
            # 删除中间的消息
            middle_index = len(self.messages) // 2
            removed = self.messages.pop(middle_index)
            
            # 添加摘要代替删除的内容
            if not hasattr(self, '_summary_added'):
                self.messages.insert(middle_index, {
                    "role": "system",
                    "content": "[之前的对话已被压缩]"
                })
                self._summary_added = True
                
            total_tokens = self._count_tokens()
```

#### 智能摘要

将长对话压缩为摘要：

```python
async def summarize_history(messages: List[Dict]) -> str:
    """将对话历史压缩为摘要"""
    prompt = f"""
    将以下对话压缩为关键信息摘要（不超过100字）：
    
    {format_messages(messages)}
    
    摘要：
    """
    return await cheap_llm.generate(prompt)  # 使用便宜的模型
```

### 9.3.5 缓存策略

### 9.3.6 语义缓存 (Semantic Cache)

对相似查询返回缓存结果：

```python
import hashlib
from sentence_transformers import SentenceTransformer

class SemanticCache:
    def __init__(self, similarity_threshold: float = 0.95):
        self.threshold = similarity_threshold
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.cache = {}  # embedding -> response
        
    def get(self, query: str) -> Optional[str]:
        query_embedding = self.encoder.encode(query)
        
        for cached_embedding, response in self.cache.items():
            similarity = cosine_similarity(query_embedding, cached_embedding)
            if similarity > self.threshold:
                return response
                
        return None
        
    def set(self, query: str, response: str):
        embedding = self.encoder.encode(query)
        self.cache[tuple(embedding)] = response
```

### 9.3.7 工具结果缓存

缓存外部 API 调用结果：

```python
from functools import lru_cache
import time

class ToolCache:
    def __init__(self, ttl_seconds: int = 3600):
        self.cache = {}
        self.ttl = ttl_seconds
        
    def cached_call(self, tool_name: str, args: Dict):
        cache_key = f"{tool_name}:{hash(frozenset(args.items()))}"
        
        if cache_key in self.cache:
            result, timestamp = self.cache[cache_key]
            if time.time() - timestamp < self.ttl:
                return result
                
        result = execute_tool(tool_name, args)
        self.cache[cache_key] = (result, time.time())
        return result
```

### 9.3.8 模型选择策略

### 9.3.9 路由器模式

根据任务复杂度选择模型：

```python
class ModelRouter:
    def __init__(self):
        self.models = {
            "simple": "gpt-5-mini",
            "medium": "gpt-5-mini", 
            "complex": "gpt-5"
        }
        
    async def route(self, query: str) -> str:
        # 使用小模型分类任务复杂度
        classification_prompt = f"""
        评估以下任务的复杂度：
        任务：{query}
        
        复杂度：simple（简单问答）/ medium（需要推理）/ complex（多步骤任务）
        只输出一个词：
        """
        
        complexity = await self.call_cheap_model(classification_prompt)
        return self.models.get(complexity.strip(), "gpt-4o-mini")
```

### 9.3.10 级联模式 (Cascade Mode)

先用便宜模型，失败再用贵模型：

```python
class CascadeAgent:
    async def process(self, query: str) -> str:
        # 第一层：尝试便宜模型
        response = await self.call_model("gpt-4o-mini", query)
        
        if self._is_high_quality(response):
            return response
            
        # 第二层：使用更强模型
        response = await self.call_model("gpt-4o", query)
        return response
        
    def _is_high_quality(self, response: str) -> bool:
        # 简单的质量检查
        return (
            len(response) > 50 and
            "不确定" not in response and
            "无法" not in response
        )
```

### 9.3.11 并行与批处理

### 9.3.12 并行调用

独立的 LLM 调用可以并行执行：

```python
import asyncio

async def parallel_process(queries: List[str]) -> List[str]:
    tasks = [process_single(q) for q in queries]
    results = await asyncio.gather(*tasks)
    return results
```

### 9.3.13 批量嵌入

嵌入调用应该批量处理：

```python
# ❌ 低效：逐条调用
for doc in documents:
    embedding = embed(doc)

# ✅ 高效：批量调用
embeddings = embed_batch(documents, batch_size=100)
```

### 9.3.14 成本监控

### 9.3.15 实时追踪

```python
class CostTracker:
    def __init__(self):
        self.costs = []
        
    def log_call(
        self, 
        model: str, 
        input_tokens: int, 
        output_tokens: int
    ):
        cost = self._calculate_cost(model, input_tokens, output_tokens)
        self.costs.append({
            "timestamp": datetime.now(),
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost
        })
        
        # 检查是否超预算
        daily_cost = self._get_daily_cost()
        if daily_cost > DAILY_BUDGET:
            raise BudgetExceededError(f"日预算已超：${daily_cost:.2f}")
```

### 9.3.16 小结

成本优化的关键策略：

| 策略 | 节省比例 | 实现难度 |
|------|----------|----------|
| Prompt 压缩 | 20-50% | 低 |
| 语义缓存 | 30-60% | 中 |
| 模型路由 | 40-70% | 中 |
| 上下文管理 | 20-40% | 中 |
| 并行处理 | 提速为主 | 低 |

**黄金法则**：
1. 先测量，再优化
2. 从最大的成本点开始
3. 质量和成本需要平衡
4. 设置预算告警，防止意外

下一节我们将探讨企业级安全与合规部署要求。

---

**下一节**: [企业级部署：架构、沙箱与安全性](9.4_enterprise.md)