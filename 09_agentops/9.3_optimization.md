## 9.3 性能优化与成本控制

智能体系统的运营成本主要来自 LLM API 调用。一个设计不当的智能体可能每个任务消耗数万 Token，导致成本失控。本节探讨如何在保证质量的前提下优化性能和控制成本。

### 9.3.1 成本构成与分析

#### 大语言模型调用成本

以“托管式大模型 API”的常见计费方式为例，成本通常由以下部分组成：
* **输入词元**：请求中提示词与上下文的计费。
* **输出词元**：模型生成内容的计费。
* **缓存/折扣机制**：部分平台对缓存命中或重复前缀提供优惠。

**典型智能体任务的词元消耗**：
一个包含多轮规划与多次工具调用的任务，词元消耗会显著高于“单次问答”。因此，成本分析应基于真实链路的采样统计，而不是只看单次调用。

#### 基础设施成本

智能体系统的配套设施成本通常不容忽视：
* **向量数据库**: 长期记忆存储和索引查询费用。
* **网络传输**: 跨区域/跨洋调用的流量与延迟成本。

#### 自建推理决策

随着规模扩大，可能需要评估从 API 转为自建推理的盈亏平衡点。建议按 TCO（算力、运维、弹性、可用性、合规、峰谷负载）做详细核算，而不是用单一阈值一刀切。

### 9.3.2 提示词与上下文优化

#### 1. 提示词压缩

减少系统提示词的冗余。

```python
# ❌ 冗余 (150 tokens)
system_prompt = "你好！你是一个非常友好和乐于助人的 AI 助手。你的主要职责是..."

# ✅ 精简 (30 tokens)
system_prompt = "角色: 专家助手. 规则: 1) 简洁. 2) 不确定时使用工具."
```

#### 2. 上下文窗口管理

除了压缩，还需要避免无限累积历史消息。使用 **滑动窗口**+**摘要** 策略。

```mermaid
graph LR
    %% Agentic Design System
    classDef user fill:#fff7e6,stroke:#fa8c16,stroke-width:2px;
    classDef agent fill:#e6f7ff,stroke:#1890ff,stroke-width:2px;
    classDef tool fill:#f6ffed,stroke:#52c41a,stroke-width:2px;

    History[历史消息: 20条] --> Trim{修剪策略}
    Trim -->|保留| Keep[首条 System + 最近 5 条]
    Trim -->|压缩| Summary[中间 14 条 -> 摘要]
    Keep & Summary --> Context[最终 Context]
    
    class History user;
    class Trim agent;
    class Keep,Summary tool;
    class Context user;
```

图 9-1：上下文滑动窗口与摘要策略

### 9.3.3 缓存策略

缓存是计算机科学领域最有效的优化手段之一，在智能体体系中也不例外。

#### 1. 提示词缓存

对于长文档分析或多轮对话，上下文的前缀往往是不变的。传统的 LLM 每次都要重新处理这些 Token。**提示词缓存** 允许将这部分计算结果缓存在 KV Cache 中，供下次请求复用。

* **工作原理**: 如果新请求的前 10k Tokens 与缓存中的一致，则直接跳过 Prefill 阶段。
* **显式控制**：部分平台支持通过请求参数显式标记可缓存段落（例如系统提示词或知识库文档）。
* **收益分析**：缓存命中通常能同时降低成本与首字延迟（TTFT），但收益取决于命中率、前缀稳定性与平台实现。

```python
# 伪代码示例：显式标记"可缓存前缀"
request = {
    "system": {
        "text": very_long_system_prompt,
        "cache_control": {"type": "ephemeral"}
    },
    "messages": [...]
}
```

#### 2. 语义缓存

对相似查询返回缓存结果，无需调用 LLM。

```mermaid
graph TD
    %% Agentic Design System
    classDef user fill:#fff7e6,stroke:#fa8c16,stroke-width:2px;
    classDef agent fill:#e6f7ff,stroke:#1890ff,stroke-width:2px;
    classDef tool fill:#f6ffed,stroke:#52c41a,stroke-width:2px;
    classDef memory fill:#fff0f6,stroke:#eb2f96,stroke-width:2px;

    Query["用户提问: '怎么重置密码?'"] --> Embed[Embedding]
    Embed --> VectorDB[(向量数据库)]
    VectorDB -->|Top-1 相似度 > 0.95| Hit{命中?}
    Hit -->|Yes| Cache["返回缓存答案: '请访问设置页面...'"]
    Hit -->|No| LLM[调用 LLM 生成]
    LLM --> CacheUpdate[更新缓存]
    
    class Query user;
    class Embed tool;
    class VectorDB memory;
    class Hit agent;
    class Cache,CacheUpdate tool;
    class LLM agent;
```

图 9-2：语义缓存工作流程

* **适用场景**: FAQ、重复性高的查询。
* **收益**：在高重复场景下，可能显著降低成本并改善延迟。

#### 3. 预加载

结合提示词缓存，在用户打开页面的瞬间，将相关的知识库上下文预加载到 KV 缓存中。

### 9.3.4 模型选择与蒸馏

**“大模型教小模型”** 是降低常态化运营成本的终极手段。

#### 1. 模型蒸馏

如果不加控制，系统为了保证效果可能会倾向于长期使用高规格模型。但对于固定场景（如“SQL 生成”或“格式化输出”），通过微调或蒸馏得到的小模型往往也能胜任。

**蒸馏流水线**:
1. **收集数据**：使用高规格模型作为“教师”运行一段时间，收集输入输出对。
2. **筛选黄金数据**：结合人工抽检与自动规则筛选出高质量样本。
3. **微调**：使用这些数据微调一个更小、更便宜的“学生”模型。
4. **替换上线**：灰度替换并回归评测，观察质量与成本的变化。

#### 2. 混合路由

即使有了微调模型，也可保留一个“兜底”机制：

```mermaid
graph LR
    %% Agentic Design System
    classDef agent fill:#e6f7ff,stroke:#1890ff,stroke-width:2px;
    classDef tool fill:#f6ffed,stroke:#52c41a,stroke-width:2px;

    Input --> Router{置信度评估}
    Router -->|高置信度| Student[低成本模型]
    Router -->|低置信度| Teacher[高规格模型]
    
    class Router agent;
    class Student tool;
    class Teacher agent;
```

图 9-3：大小模型混合路由策略

### 9.3.5 延迟优化

要降低延迟，架构层面的优化往往比代码层面的微调更有效。

#### 1. 分离式架构 （Disaggregated Serving）

对于自建模型或私有化部署，**预填充（Prefill）与解码（Decode）的分离**是解决高并发延迟的关键架构。
* **物理瓶颈互斥**: 计算处理极长的输入提示词（Prefill 阶段）对 GPU 是计算密集型的；而逐字生成答案（Decode 阶段）则因为参数搬运缓慢而属于访存密集型。混合部署会导致突发长 Prompt 请求瞬间霸占所有算力，引发其他并发对话被严重卡顿（队头阻塞）。
* **解耦方案**: 领军系统将机器拆分为独立的 **Prefill 集群**（主打算力）和 **Decode 集群**（主打显存带宽），两者通过超敏捷的高速 RDMA 网络在微秒级实现海量 KV Cache 状态的跨节点传输，彻底抚平算力与显存毛刺。

#### 2. 投机解码 （Speculative Decoding）

打破自回归模型必须“串行龟速生成”的宿命，尤其适合消除智能体冗长思维链过程的时间消耗。
* **机制**: 引入一个极其小巧且飞快的“草稿模型”（或并行的辅助预测头），它先快速猜想出后续的 5 个候选 Token，接着交由庞大的“目标模型”在一个并行批次中同时验证。
* **收益**: 因为大模型在 Decode 时其实多数算力处于闲置，并行验证多个 Token 的耗时与生成 1 个 Token 基本相同。只要目标模型接受了草稿输出，就等于在零时间成本下“免费”生成了多个字。在完全不损失生成智商的前提下，能将整个任务时延降低几倍。

#### 3. 并行推测

在智能体的应用层，可以预判工具调用结果并提前触发下游操作：
* **预判分支**: 如果 Agent 大概率会调用某个 API，可以在等待 LLM 生成最终决策的同时，提前异步发起该 API 请求。
* **并行工具调用**: 当多个工具调用之间无依赖关系时，并行执行而非串行等待。

#### 4. 网络优化

* **跨地域延迟**：使用边缘节点或多区域部署来减少跨地域联网的 RTT。
* **协议优化**: 使用 HTTP/2 或 WebSocket 减少连接建立开销。

### 9.3.6 成本监控

FinOps 是一种将财务问责制引入云和 AI 支出的文化与实践。在智能体体系中，它意味着工程团队需要对每一次 Token 消耗的 ROI 负责。

1. **预算告警**: 设置每日/每月硬性预算。
2. **归因分析**: 给每个 Trace 打上 `User_ID`, `Feature_ID` 标签，分析哪个功能最烧钱。
3. **异常检测**: 监控 `Cost / Request` 指标，发现异常突增（通常意味着死循环）。

---

**下一节**: [9.4 企业级安全与合规部署](9.4_enterprise.md)
