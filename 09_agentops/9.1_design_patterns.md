## 9.1 设计模式：从 Workflow 到 Agent

Anthropic 在构建大量智能体的实践中发现，最成功的系统往往不是用最复杂的框架构建的，而是使用了简单、可组合的模式。我们将这些模式统称为 **Agentic Systems**，并将其在架构上明确分为两类：**Workflows (工作流)** 和 **Agents (智能体)**。

- **工作流 (Workflows)**: 系统通过预定义的代码路径编排 LLM 和工具。路径是固定的，确定性强。
- **智能体 (Agents)**: LLM 动态控制流程和工具使用，拥有决策权。适应性强，但不可控性增加。

本节将详细解析这两种架构下的核心设计模式。

### 9.1.1 Workflows：确定性编排

当任务可以被清晰分解时，应优先使用 Workflow。它们能提供更高的可预测性。

#### 1. 提示词链（Prompt Chaining）
最基础的模式。将任务分解为一系列线性步骤，上一步的输出作为下一步的输入。
- **适用场景**：文案生成后翻译、生成大纲后扩写。
- **优势**：极低的延迟，易于调试。

#### 2. 路由（Routing）
根据输入内容的分类，将其导向不同的后续流程。
- **结构**：Input -> Router (Classifier) -> Branch A/B/C
- **适用场景**：客服分流（售后/投诉）、难易分流（小模型处理简单题）。

#### 3. 并行化（Parallelization）
利用 LLM 并行处理能力，提升速度或质量。
- **切片（Sectioning）**：将大任务切成独立的子任务并行处理（如同时审查代码的安全性、性能、风格）。
- **投票（Voting）**：同一任务运行多次，通过投票减少幻觉（如让 3 个模型分别找 Bug，取交集）。

#### 4. 编排者-工作者（Orchestrator-Workers）
由于子任务无法预知（如"修改某个功能涉及哪几个文件"），需要一个中心 LLM (Orchestrator) 动态分析任务，然后分派给具体的 Worker LLMs 执行。
- **适用场景**：复杂编码任务、多源信息调研。
- **区别**：与传统的 Planner 模式类似，但更强调 Orchestrator 在执行过程中的动态分派与结果汇总（Synthesize）。

#### 5. 评估者-优化者（Evaluator-Optimizer）
即经典的 **Reflection (反思)** 模式。
- **流程**：Generator 生成初稿 -> Evaluator 提出修改意见 -> Generator 优化 -> 循环。
- **适用场景**：高质量翻译、复杂代码生成。人类作家写书也是这个过程。

### 9.1.2 Agents：动态决策

当无法预定义路径时，我们需要让 LLM 接管控制权。

#### 1. ReAct & Tool Use
智能体处于一个循环中：思考 (Thought) -> 调用工具 (Action) -> 观察结果 (Observation)。
- **核心**：智能体必须从环境中获取 **基本事实 (Ground Truth)** 来修正下一步行动。这也是 **简洁 (Simplicity)** 原则的体现。

#### 2. 自主智能体（Autonomous Agents）
在受信任的环境中（如沙箱），赋予智能体更高的自主权。它不仅执行单步指令，还能自行规划长序列操作，甚至在遇到错误时自我恢复。
- **关键**：需要极强的 **工具定义 (Tool Definition)** 和 **防错设计 (Poka-yoke)**。
- **适用场景**：开放式探索（"找出系统中的所有漏洞"）、长期运行的任务。

### 9.1.3 如何选择

Anthropic 建议遵循 **"Simplicity First"** 原则：

1. **从 Prompt Chaining 开始**：如果能用简单的 Prompt 链解决，就不要用智能体。
2. **需要决策时用 Routing**。
3. **任务复杂时先试 Orchestrator-workers**。
4. **只有在需要极高灵活性和处理开放世界问题时，才使用 Autonomous Agents**。

增加复杂性意味着增加延迟和成本，必须确保它能带来相应的效果提升。

### 9.1.4 模式与框架的对应

| 模式 | LangGraph 实现 | AutoGen 实现 | 备注 |
| :--- | :--- | :--- | :--- |
| **Routing** | Conditional Edges | GroupChat Manager | 基础控制流 |
| **Parallelization** | Map-Reduce (Fan-out) | - | 提升效率 |
| **Orchestrator** | StateGraph (Supervisor) | Two-Agent / GroupChat | 解决复杂任务首选 |
| **Evaluator** | Cycle Graph (A <-> B) | Nested Chat | 提升质量首选 |
| **Agent** | `prebuilt.create_react_agent` | `AssistantAgent` | 动态应对未知 |

智能体设计的艺术，就在于根据业务场景，灵活组合这些模式。例如，你完全可以构建一个 Workfow，其中某一步是一个 ReAct 智能体，做完后进入 Evaluator-Optimizer 环节。

#### 行业最佳实践

随着 Agentic AI Foundation (AAIF) 的成立和行业标准化进程的推进，各大 AI 厂商也总结出了一套成熟的 Agent 设计最佳实践。

#### Anthropic 的 Agent 设计原则

Anthropic 在 Claude 智能体 SDK 中推广以下设计模式：

1. **核心智能体循环**
   ```
   收集上下文 → 采取行动 → 验证工作 → 重复
   ```

2. **给 Claude 一台"计算机"**
   - 提供 bash 命令执行能力
   - 提供文件编辑和创建能力
   - 提供 Web 访问能力

3. **智能体技能（Agent Skills）**
   - 将领域专业知识封装为可动态加载的"技能"
   - 技能包含指令、脚本和资源文件
   - 智能体可根据任务需求自动发现和加载技能

4. **CLAUDE.md 项目记忆**
   - 类似 AGENTS.md，用于存储项目约定和架构笔记
   - 作为 Claude 的持久化参考文档

#### OpenAI 的 Guardrails 最佳实践

OpenAI 在其智能体开发指南中强调"护栏"的重要性：

1. **分层护栏**
   - **输入层**：验证和过滤用户输入
   - **行为层**：限制智能体可执行的操作范围
   - **输出层**：审计和过滤智能体输出

2. **人类监督机制**
   - 高风险操作需人工确认
   - 敏感任务触发审批流程
   - 拒绝执行明显危险操作

3. **最小权限原则**
   - 只授予智能体完成任务所需的最小权限
   - 限制数据源访问范围
   - 沙箱化执行环境

### 9.1.5 Google 的 Agent 安全指南

Google Cloud 在智能体开发工具包 (ADK) 中集成了以下安全机制：

1. **模型装甲（Model Armor）**
   - 防止 Prompt 注入攻击
   - 检测数据泄露风险
   - 防范工具投毒（Tool Poisoning）

2. **自动发现与保护**
   - 自动发现运行中的智能体和 MCP 服务器
   - 识别潜在漏洞
   - 提供实时保护

3. **可观测性**
   - 全链路追踪智能体执行过程
   - 监控异常行为
   - 记录审计日志

### 9.1.6 跨厂商共识

尽管各厂商有不同的实现细节，但在以下原则上达成了共识：

| 原则 | 说明 |
|------|------|
| **透明性** | 智能体的决策过程应可解释和可追溯 |
| **可控性** | 人类应能随时介入和终止智能体行为 |
| **安全性** | 默认采用最小权限，敏感操作需确认 |
| **可审计** | 所有操作应记录日志以供审计 |
| **互操作** | 遵循开放标准（如 MCP）实现跨平台协作 |

> [!TIP]
> 建议参考 AAIF 官网 (https://aaif.io) 获取最新的行业标准和最佳实践文档。

### 9.1.7 系统设计优先于模型能力

2025 年发表的研究《[Why Do Multi-Agent LLM Systems Fail?](https://arxiv.org/abs/2503.13657)》通过分析 1642 条执行轨迹，揭示了一个反直觉的发现：**多智能体系统的失败更多源于组织设计缺陷，而非模型能力不足**。

#### 关键数据

- 7 个主流 MAS 框架的失败率在 **41%-86.7%** 之间
- 在相同模型（GPT-4o）下，MetaGPT 比 ChatDev 在系统设计问题（FC1）和协调问题（FC2）上减少了 **60-68%** 的失败
- 仅通过优化 Prompt 设计（不更换模型），可提升 **15.6%** 的任务成功率

#### 设计启示

这一发现与组织行为学的研究一致：即使是由能力出众的个体组成的团队，如果组织结构存在缺陷，也可能导致灾难性失败。对于智能体系统设计者而言：

1. **先优化架构，再考虑换模型**：在更换更强模型之前，先检查系统设计是否存在问题
2. **明确工作流边界**：清晰定义每个智能体的输入输出规范和终止条件
3. **引入验证环节**：在关键步骤后增加验证智能体，减少错误传播

> [!IMPORTANT]
> 投资于系统设计的改进往往比升级模型更具成本效益。

### 9.1.8 Cursor 智能体开发实践

Cursor 团队在 2026 年初分享了一套经过大规模验证的智能体开发最佳实践。

#### 计划模式（Plan Mode）工作流

按 `Shift+Tab` 切换到 Plan Mode，智能体会：

1. 研究代码库以找到相关文件
2. 就需求提出澄清问题
3. 创建包含文件路径和代码引用的详细实施计划
4. 等待你批准后再开始构建

```markdown
# Plan Mode 的价值
- 计划以 Markdown 文件打开，可直接编辑
- 移除不必要的步骤、调整方法、添加智能体遗漏的上下文
- 点击"保存到工作区"将计划存储在 .cursor/plans/ 中
```

**从计划重新开始**：当智能体构建的东西不符合预期时，与其通过跟进提示修复，不如回退更改、细化计划、重新运行。这通常比修复进行中的智能体更快，结果更干净。

#### 上下文管理策略

**让智能体自己找上下文**：

不需要手动标记每个文件。Cursor 智能体有强大的搜索工具，按需拉取上下文。当你询问"认证流程"时，智能体会通过 `grep` 和语义搜索找到相关文件。

**何时开启新对话 vs 继续对话**：

| 开启新对话 | 继续对话 |
|-----------|---------|
| 切换到不同任务或功能 | 迭代同一功能 |
| 智能体看起来困惑或重复犯错 | 智能体需要早期对话的上下文 |
| 完成一个逻辑工作单元 | 调试它刚构建的东西 |

> **长对话会导致智能体失去焦点**。经过多轮对话和摘要后，上下文积累噪声，智能体可能分心或切换到无关任务。

**引用过去的工作**：开始新对话时，使用 `@Past Chats` 引用之前的工作，而不是复制粘贴整个对话。智能体可以选择性地从聊天历史中读取所需的上下文。

#### Rules 与 Skills 配置

**Rules（静态上下文）**：在 `.cursor/rules/` 中创建 Markdown 文件：

```markdown
# Commands
- `npm run build`: 构建项目
- `npm run typecheck`: 运行类型检查
- `npm run test`: 运行测试（优先单个测试文件以提速）

# Code style
- 使用 ES modules (import/export)，不用 CommonJS (require)
- 尽可能解构导入：`import { foo } from 'bar'`
- 参见 `components/Button.tsx` 作为规范组件结构

# Workflow
- 做完一系列代码更改后总是运行类型检查
- API 路由放在 `app/api/` 并遵循现有模式
```

**Skills（动态能力）**：与 Rules 不同，Skills 在智能体判断相关时才动态加载。Skills 可以包含：
- 自定义命令：可复用的工作流，通过 `/` 触发
- Hooks：在智能体动作前后运行的脚本
- 领域知识：智能体可按需拉取的特定任务指令

#### 调试模式（Debug Mode）

当标准 Agent 交互难以解决 bug 时，Debug Mode 提供不同的方法：

```
Debug Mode 工作流程：
1. 生成多个关于可能出错的假设
2. 用日志语句检测你的代码
3. 要求你重现 bug 同时收集运行时数据
4. 分析实际行为以定位根因
5. 基于证据进行定向修复
```

**最适合的场景**：
- 可重现但无法弄清的 bug
- 竞态条件和时序问题
- 性能问题和内存泄漏
- 曾经正常的回归问题

#### 并行智能体与 Git Worktree

Cursor 可以轻松运行多个并行智能体而不相互干扰：

- **原生 Worktree 支持**：每个智能体在自己的 worktree 中运行，文件和更改隔离
- **多模型并行**：同一提示同时运行多个模型，并排比较结果
- **最佳方案选择**：Cursor 建议它认为最好的解决方案

**适用场景**：
- 不同模型可能采用不同方法的困难问题
- 比较不同模型家族的代码质量
- 发现单个模型可能遗漏的边缘情况

#### TDD 与智能体

智能体在有明确目标可迭代时表现最佳。测试允许智能体做出更改、评估结果、逐步改进直到成功：

```markdown
TDD 工作流：
1. 让智能体根据预期输入/输出编写测试
2. 告诉智能体运行测试并确认失败（不要写实现！）
3. 满意后提交测试
4. 让智能体编写通过测试的代码（不要修改测试！）
5. 满意后提交实现
```

### 9.1.9 复利工程（Compounding Engineering）

传统的 AI 编程是线性的 (Prompt -> Code)，而 **复利工程 (Compounding Engineering)** 致力于构建具有记忆的系统。

#### 核心循环
1.  **Issue**: 遇到 Bug 或 Bad Case。
2.  **Fix**: 修复代码。
3.  **Learn**: 问自己“如何防止下次再犯？”
    *   更新 `AGENTS.md`（项目知识库）。
    *   添加 Linter 规则。
    *   更新 System Prompt。

通过这种方式，智能体的每一次错误都成为系统进化的养料。三个月后，你的系统将包含成百上千条沉淀下来的“隐形知识”，这是通用模型无法比拟的护城河。

### 9.1.10 小结

- 简单的任务用 **Router** + **ReAct**。
- 复杂的任务用 **Planner**。
- 要求高质量的任务用 **Reflection**。
- 生产级部署遵循**行业最佳实践**和**安全护栏**。

下一节我们将探讨如何对这些复杂的智能体系统进行监控和调试。

---

**下一节**: [可观测性 (Observability) 与调试](9.2_observability.md)