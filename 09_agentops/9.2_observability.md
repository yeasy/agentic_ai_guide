## 9.2 可观测性 与调试

在传统的软件开发中，我们有 Stack Trace 报错堆栈。但在 Agent 开发中，"Bug" 往往不是代码崩溃，而是 Agent "一本正经地胡说八道" 或者 "陷入了无意义的死循环"。

当 Agent 出错时，你不能只看最后一行输出。你需要像黑匣子一样，记录下它每一次思考、每一次工具调用和每一次状态更新。这就是 Agent 的可观测性。

#### 为什么 Agent 难以调试

1.  **非确定性 (Nondeterminism)**：同样的 Prompt，上午跑是通的，下午跑就挂了。
2.  **黑盒推理**：LLM 为什么决定调用 Search 而不是 Calculator？
3.  **多步级联**：第 5 步的错误可能是第 1 步的一个微小偏差导致的蝴蝶效应。
4.  **成本隐患**：一个死循环的 Agent 可能在一夜之间烧掉几千美元。

### 9.2.1 核心指标：Trace, Span 与 Run

借鉴分布式追踪（Distributed Tracing）的概念，我们这样定义 Agent 的监控数据：

- **Trace (链路)**：代表一次完整的用户请求处理全过程（Request）。
- **Run / Span (跨度)**：链路中的每一个原子操作。例如：
  - 一次 LLM 调用 (LLM Run)
  - 一次工具执行 (Tool Run)
  - 一次检索 (Retriever Run)
  - 一次子链执行 (Chain Run)

### 9.2.2 一个典型的 Trace 结构
```text
Trace ID: 890s-f9s8-s9d8
  ├── [Chain] Main Agent (Duration: 5s, Cost: $0.02)
  │    ├── [LLM] Planner: "我要先查天气..."
  │    ├── [Tool] Search("Beijing weather")
  │    │    └── [Output] "Sunny, 25C"
  │    ├── [LLM] Generator: "今天北京天气不错..."
  │    └── [Output] Final Answer
```

### 9.2.3 主流可观测性工具平台

#### LangSmith

目前体验最好的 AgentOps 平台。

- **核心功能**：
  - **Tracing**：可视化展示每一步的输入输出。
  - **Playground**：在 Trace 界面直接点击 "Edit"，进入 Playground 修改 Prompt 并重跑这一步。这是调试 Prompt 最快的方法。
  - **Dataset**：将线上的 Trace 一键添加到测试数据集。
  
- **代码集成**：
  只要设置环境变量，LangChain 应用会自动上报数据。
  ```bash
  export LANGCHAIN_TRACING_V2=true
  export LANGCHAIN_API_KEY=ls-...
  ```

#### Arize Phoenix (开源)

如果是本地开发或对数据隐私敏感，Phoenix 是极佳的开源选择。

- **特点**：支持本地启动 Web UI，支持对 Embedding 检索质量的可视化分析。
- **适用**：RAG 应用的深度调试。

#### Weights & Biases

从传统的 ML 训练监控切入 Agent 监控。

- **特点**：强大的 Prompt 版本管理（Prompts Registry）。适合需要多人协作调优 Prompt 的团队。

### 9.2.4 鲁棒性工程：防崩溃指南

除了监控，我们要通过代码设计防止 Agent "发疯"。

#### 循环熔断

Agent 本质是 `while` 循环，必须防止它无限空转。

```python
# 必须设置最大步数
agent = create_react_agent(
    model, 
    tools, 
    max_iterations=15  # 超过 15 步强制停止
)

# 必须设置超时
if time.time() - start_time > 60:
    raise TimeoutError("任务执行超时")
```

#### 输出校验

永远不要相信 LLM 输出的 JSON。

- **Pydantic Validation**：
  使用 Pydantic 定义期望的数据结构。如果 LLM 少了字段，Parser 会报错。
  
- **Auto-Fixing Parser**：
  捕获 Validation Error，把它作为一条新的 System Message 发回给 LLM："你上一步输出的 JSON 格式不对，缺少了 'action' 字段，请修正。" 
  通常 LLM 都能在下一轮自我修正。

#### 错误捕获与反馈

当工具调用失败（如网络超时 500）时，**千万不要抛出异常导致程序崩溃**。

正确的做法是捕获异常，并将其转化为一段自然语言的 **Observation** 反馈给 Agent。

```python
def safe_tool_run(query):
    try:
        return api.search(query)
    except Exception as e:
        # Agent 看到这个不仅不会挂，还会尝试换个词搜索
        return f"系统提示：搜索工具调用失败，错误信息：{e}。请尝试缩短关键词重试。"
```

### 9.2.5 成本监控与预算

在企业级应用中，Token 就是钱。

- **Token 计数器**：使用 `tiktoken` 库在每次请求前预估 Cost。
- **预算控制**：为每个 Trace 设置 Budget。
  ```python
  with get_openai_callback() as cb:
      agent.run()
      if cb.total_cost > 1.0:
          stop_processing()
  ```

### 9.2.6 小结

可观测性不仅仅是为了修 Bug，更是为了**理解**。
通过观察 Trace，你会发现 80% 的"不够智能"其实是因为：
1. 检索到的文档不对（RAG 问题）。
2. System Prompt 描述不清（Prompt Engineering 问题）。
3. 工具报错后 Agent 不知所措（鲁棒性问题）。

拥有了可观测性，你才能从"不知所措"变成"有的放矢"。

下一节我们将探讨如何优化性能并控制成本。

---

**下一节**: [性能优化与成本控制](9.3_optimization.md)