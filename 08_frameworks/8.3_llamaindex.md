## 8.3 LlamaIndex：数据驱动的 RAG 专家

[LlamaIndex](https://www.llamaindex.ai/) 是专注于数据连接和检索增强生成（RAG）的框架。虽然它以 RAG 能力闻名，但 LlamaIndex 也提供了强大的智能体构建能力，特别适合需要处理大量非结构化数据的智能体应用。

### 8.3.1 核心哲学

LlamaIndex 的设计理念是 **"以数据为中心的 AI"（Data-Centric AI）**：

```
传统智能体:  用户 → LLM → 工具 → 结果
LlamaIndex:  用户 → LLM → 数据索引 → 知识检索 → 结果
```

### 8.3.2 与 LangChain 的区别

| 维度 | LangChain/LangGraph | LlamaIndex |
|------|---------------------|------------|
| 核心关注 | 工作流编排 | 数据索引与检索 |
| 强项 | 复杂控制流 | RAG 与知识管理 |
| 智能体模式 | 通用智能体 | 数据智能体 |
| 适用场景 | 多步骤任务 | 知识密集型任务 |

### 8.3.3 核心组件：索引（Index）

LlamaIndex 的核心是各种索引类型：

```python
from llama_index.core import VectorStoreIndex, Document

# 从文档创建向量索引

documents = [
    Document(text="人工智能的历史可以追溯到1950年代..."),
    Document(text="机器学习是人工智能的一个分支..."),
]

index = VectorStoreIndex.from_documents(documents)
```

**索引类型**：

| 类型 | 用途 | 适用场景 |
|------|------|----------|
| 向量存储索引 (VectorStoreIndex) | 语义搜索 | 通用问答 |
| 摘要索引 (SummaryIndex) | 全文摘要 | 文档总结 |
| 树索引 (TreeIndex) | 层级检索 | 结构化文档 |
| 知识图谱索引 (KnowledgeGraphIndex) | 知识图谱 | 实体关系查询 |

### 8.3.4 查询引擎（Query Engine）

查询引擎是索引的查询接口：

```python
# 创建查询引擎

query_engine = index.as_query_engine(
    similarity_top_k=5,
    response_mode="tree_summarize"
)

# 执行查询

response = query_engine.query("什么是机器学习？")
print(response)
```

### 8.3.5 高级 RAG 策略

具体示例如下：

```python
from llama_index.core.postprocessor import (
    SimilarityPostprocessor,
    LongContextReorder
)

query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[
        SimilarityPostprocessor(similarity_cutoff=0.7),
        LongContextReorder()  # 重排序优化长上下文
    ]
)
```

### 8.3.6 基本智能体

LlamaIndex 提供了将查询引擎作为工具的能力：

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool

# 将查询引擎封装为工具

query_engine_tool = QueryEngineTool.from_defaults(
    query_engine=query_engine,
    name="knowledge_base",
    description="用于查询公司内部知识库的工具"
)

# 添加其他工具

tools = [
    query_engine_tool,
    search_web_tool,
    calculator_tool
]

# 创建智能体

agent = ReActAgent.from_tools(
    tools=tools,
    llm=llm,
    verbose=True
)

# 使用

response = agent.chat("公司的休假政策是什么？")
```

### 8.3.7 多索引智能体

处理多个数据源的智能体：

```python
# 为不同数据源创建索引

hr_index = VectorStoreIndex.from_documents(hr_docs)
finance_index = VectorStoreIndex.from_documents(finance_docs)
tech_index = VectorStoreIndex.from_documents(tech_docs)

# 创建多个查询引擎工具

tools = [
    QueryEngineTool.from_defaults(
        query_engine=hr_index.as_query_engine(),
        name="hr_knowledge",
        description="人力资源相关问题：休假、福利、考勤"
    ),
    QueryEngineTool.from_defaults(
        query_engine=finance_index.as_query_engine(),
        name="finance_knowledge", 
        description="财务相关问题：报销、预算、账务"
    ),
    QueryEngineTool.from_defaults(
        query_engine=tech_index.as_query_engine(),
        name="tech_knowledge",
        description="技术文档：API、架构、部署"
    )
]

# 智能体会自动选择合适的知识库

agent = ReActAgent.from_tools(tools, llm=llm)
```

### 8.3.8 子问题查询引擎

自动将复杂问题分解为子问题：

```python
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.tools import QueryEngineTool, ToolMetadata

# 创建子问题查询引擎

sub_question_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=[
        QueryEngineTool(
            query_engine=sales_engine,
            metadata=ToolMetadata(
                name="sales_data",
                description="销售数据查询"
            )
        ),
        QueryEngineTool(
            query_engine=marketing_engine,
            metadata=ToolMetadata(
                name="marketing_data",
                description="营销数据查询"
            )
        )
    ],
    llm=llm
)

# 复杂问题会被自动分解

response = sub_question_engine.query(
    "对比去年和今年的销售额，分析营销活动的效果"
)
# 内部会生成子问题：

# 1. 去年的销售额是多少？

# 2. 今年的销售额是多少？

# 3. 去年的营销活动有哪些？

# 4. 今年的营销活动有哪些？

# 然后综合回答
```

### 8.3.9 实战：文档分析智能体

#### 场景描述

构建一个能够分析多种格式文档（PDF、Word、Excel）的智能助手。

#### 实现代码

具体示例如下：

```python
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings
)
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool, FunctionTool
from llama_index.llms.openai import OpenAI

# 1. 配置

Settings.llm = OpenAI(model="gpt-4")
Settings.embed_model = "text-embedding-3-small"

# 2. 加载文档

documents = SimpleDirectoryReader(
    input_dir="./documents",
    recursive=True,
    required_exts=[".pdf", ".docx", ".xlsx", ".txt"]
).load_data()

# 3. 创建索引

index = VectorStoreIndex.from_documents(documents)

# 4. 定义工具

@FunctionTool.from_defaults
def summarize_document(doc_name: str) -> str:
    """生成指定文档的摘要"""
    # 过滤特定文档

    doc_index = VectorStoreIndex.from_documents(
        [d for d in documents if doc_name in d.metadata.get("file_name", "")]
    )
    engine = doc_index.as_query_engine(response_mode="tree_summarize")
    return engine.query("请总结这个文档的主要内容").response

@FunctionTool.from_defaults  
def compare_documents(doc1: str, doc2: str, aspect: str) -> str:
    """对比两个文档在某个方面的差异"""
    query = f"对比 {doc1} 和 {doc2} 在 {aspect} 方面的差异"
    return index.as_query_engine().query(query).response

# 5. 创建智能体

tools = [
    QueryEngineTool.from_defaults(
        query_engine=index.as_query_engine(),
        name="document_search",
        description="搜索文档内容，回答关于文档的问题"
    ),
    summarize_document,
    compare_documents
]

agent = ReActAgent.from_tools(
    tools=tools,
    verbose=True,
    max_iterations=10
)

# 6. 使用

response = agent.chat(
    "帮我总结一下合同文档，并对比与上一版的主要变化"
)
```

### 8.3.10 与向量数据库集成

#### Chroma 集成

具体示例如下：

```python
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext

# 创建 Chroma 客户端

chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("documents")

# 创建向量存储

vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# 创建索引

index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)
```

#### Pinecone 集成

具体示例如下：

```python
from pinecone import Pinecone
from llama_index.vector_stores.pinecone import PineconeVectorStore

pc = Pinecone(api_key="your-api-key")
index = pc.Index("my-index")

vector_store = PineconeVectorStore(pinecone_index=index)
```

### 8.3.11 小结

LlamaIndex 的核心优势：

- **数据连接能力**：支持数十种数据源格式
- **灵活的索引**：多种索引类型满足不同需求
- **RAG 优化**：内置多种检索和后处理策略
- **Query Engine as Tool**：将知识库封装为智能体工具

适用场景：
- 企业知识库问答
- 文档分析和比较
- 数据驱动的决策支持
- 需要处理大量非结构化数据的应用

下一节将探讨多智能体框架 AutoGen 和 CrewAI。

---

**下一节**: [多体协作：AutoGen 与 CrewAI 实战](8.4_multi_agent.md)
