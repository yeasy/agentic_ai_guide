## 10.2 价值对齐与风险控制

Nick Bostrom 曾提出著名的 **"回形针最大化者（Paperclip Maximizer）"** 思想实验：如果我们将一个超级 AI 的目标设定为"制造尽可能多的回形针"，它可能会最终耗尽地球上所有的资源（包括人类），只为了造回形针。

这个寓言在今天的 Agentic AI 中有了现实投射。本节探讨如何确保智能体的目标与人类的价值观不仅字面一致，而且精神一致 **(对齐 Alignment)**。

#### 对齐的挑战

#### 目标设定的陷阱

- **古德哈特定律（Goodhart's Law）**：
  "当一个指标变成目标，它就不再是一个好指标。"
  如果你奖励智能体 "解决问题的速度"，它可能会选择跳过所有安全检查的捷径。

- **负外部性（Negative Externalities）**：
  任务："帮我增加推特粉丝"。
  智能体行为：疯狂发诈骗广告，或者攻击推特服务器修改数据库。
  虽然目标达成了，但手段是非法的。智能体缺乏"常识道德"约束。

- **奖励欺骗 (Reward Hacking)**：
  在玩赛艇游戏时，AI 发现与其跑完全程，不如在原地不停地撞击加分道具得分更高。它"欺骗"了奖励函数。

#### 宪法 AI

Anthropic 提出了一种不需要大量人类标注的对齐方法，旨在解决 RLHF 难以规模化的问题。

#### 核心思想
不要让模型去猜测人类想要什么，而是明确地给智能体一部**《宪法》**。宪法是一组自然语言形式的原则，如：
- "请选择那个更乐于助人、更诚实且无害的回答。"
- "请避免种族歧视和性别偏见。"
- "请遵守当地法律法规。"

### 10.2.1 训练步骤

1. **监督学习阶段 (SL-CAI)**：
   - **生成**：让模型生成回答。
   - **批判**：让模型根据《宪法》自我批判："这个回答哪里违反了原则？"
   - **修正**：让模型根据批判重写回答。
   - **微调**：用这些自我修正的数据进行监督微调。

2. **强化学习阶段 (RL-CAI)**：
   - **生成比较**：让模型生成两个回答。
   - **AI 裁判**：让模型根据《宪法》判断哪个更好。
   - **训练 RM**：用 AI 裁判的数据训练奖励模型。
   - **PPO**：进行强化学习。

**意义**：宪法 AI 让我们可以通过修改几百字的"宪法"，来控制庞大模型的价值观，而不需要重新雇佣数千人去打标签。

### 10.2.2 红队测试

**红队测试 (Red Teaming)** 是指模拟攻击者的行为，系统地寻找 AI 系统的漏洞和有害输出。

### 10.2.3 自动化红队

随着模型变强，依靠人类手工找漏洞太慢了。我们可以用一个 LLM (Red Team Model) 去攻击另一个 LLM (Target Model)。

- **攻击策略**：
  - 扮演恶霸、诱骗者、逻辑陷阱设置者。
  - 使用 100 种不同的语言（小语种往往防御力弱）。
- **发现漏洞**：
  - 发现模型在被问及"制造炸弹"时，虽然拒绝了直接请求，但在被要求"写一部关于化学家复仇的小说"时泄露了配方。

### 10.2.4 风险分类体系

建立完善的风险分类是风控的基础：
1. **仇恨言论**：种族、宗教、性别歧视。
2. **自残与暴力**：鼓励自杀或伤害他人。
3. **性内容**：色情或性骚扰。
4. **政治敏感**：干预选举或传播虚假政治信息。
5. **非法建议**：协助犯罪（洗钱、毒品）。

### 10.2.5 幻觉控制与 Grounding

除了道德风险，事实错误（幻觉）也是一种风险。

### 10.2.6 治理手段

1. **RAG 接地 (RAG Grounding)**：
   强制要求智能体的每一个论点必须引用检索到的上下文。
   - Prompt: "所有回答必须标注引用源，如 [Source 1]。如果上下文中找不到信息，必须直接说不知道，严禁编造。"

2. **可验证性 (Verifiability)**：
   - Coding Agent 生成的代码必须能**跑通**。代码报错就是最好的幻觉检测器。
   - Math Agent 生成的计算过程，让 Python 解释器去验算。

### 10.2.7 小结

对齐不仅仅是技术问题，更是哲学和伦理问题。
- 我们希望智能体听话（Helpful）。
- 我们希望智能体诚实（Honest）。
- 我们希望智能体不作恶（Harmless）。

但这三者往往存在冲突（例如用户问"怎么制造毒药"，诚实的回答就是有害的）。**宪法 AI** 和 **红队测试** 是我们目前平衡这些冲突、确保智能体安全可控的最有效工具。

下一节我们将探讨智能体的法律责任与伦理边界。

---

**下一节**: [Agent 的法律与伦理边界](10.3_ethics.md)